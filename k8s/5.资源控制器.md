1.1 什么是控制器

Kubernetes 中内建了很多controller（控制器），这些相当于一个状态机，用来控制 Pod 的具体状态和行为。

1.1.1 pod的分类

- 自主式Pod，不受控制器控制

- 控制器管理的Pod；在控制器的管理周期力，始终维持Pod的数量

1.2 控制器类型

- ReplicationController 和 ReplicaSet

- Deployment

- DaemonSet

- StateFulSet

- Job/CronJob

- Horizontal Pod Autoscaling

1.2.1 ReplicationController 和 ReplicaSet

ReplicationController（RC）用来确保容器应用的副本数始终保持在用户定义的副本数，即如果有容器异常退 出，会自动创建新的Pod来替代；而如果异常多出来的容器也会自动回收； 在新版本的 Kubernetes 中建议使用 ReplicaSet 来取代 ReplicationController 。ReplicaSet 跟 ReplicationController没有本质的不同，只是名字不一样，并且 ReplicaSet 支持集合式的 selector；

1.2.2 Deployment

Deployment 为 Pod 和 ReplicaSet 提供了一个声明式定义 (declarative) 方法，用来替代以前的 ReplicationController 来方便的管理应用。典型的应用场景包括；

- 定义 Deployment 来创建 Pod 和 ReplicaSet

- 滚动升级和回滚应用

- 扩容和缩容

- 暂停和继续 Deployment

 区分命令式：rc 、creat 声明式：apply、rs 

注意：deployment 通过RS来管理pod

 ![img](https://img-blog.csdnimg.cn/20200611195836538.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDgwMDkxNQ==,size_16,color_FFFFFF,t_70) 

1.2.3 DaemonSet

DaemonSet 确保全部（或者一些）Node 上运行一个 Pod 的副本。当有 Node 加入集群时，也会为他们新增一个 Pod 。当有 Node 从集群移除时，这些 Pod 也会被回收。删除 DaemonSet 将会删除它创建的所有 Pod使用 DaemonSet 的一些典型用法：

- 运行集群存储 daemon，例如在每个 Node 上运行 glusterd 、 ceph

- 在每个 Node 上运行日志收集 daemon，例如 fluentd 、 logstash

- 在每个 Node 上运行监控 daemon，例如 Prometheus Node Exporter、 collectd 、Datadog 代理、 New Relic 代理，或 Ganglia gmond

1.2.4 Job

Job 负责批处理任务，即仅执行一次的任务，它保证批处理任务的一个或多个 Pod 成功结束

1.2.5 CronJob

Cron Job 管理基于时间的 Job，即：

- 在给定时间点只运行一次

- 周期性地在给定时间点运行

```shell
使用前提条件：当前使用的 Kubernetes 集群，版本 >= 1.8（对 CronJob）。对于先前版本的集群，版本 < 1.8，启动 API Server时，通过传递选项  --runtime-config=batch/v2alpha1=true  可以开启 batch/v2alpha1 API 中文介绍：https://kuboard.cn/learning/k8s-intermediate/workload/wl-cronjob/run.html#创建cronjob
```

典型的用法如下所示：

- 在给定的时间点调度 Job 运行

- 创建周期性运行的 Job，例如：数据库备份、发送邮件

1.2.6 StatefulSet

StatefulSet 作为 Controller 为 Pod 提供唯一的标识。它可以保证部署和 scale 的顺序 StatefulSet是为了解决有状态服务的问题（对应Deployments和ReplicaSets是为无状态服务而设计），其应用 场景包括：

- 稳定的持久化存储，即Pod重新调度后还是能访问到相同的持久化数据，基于PVC来实现

- 稳定的网络标志，即Pod重新调度后其PodName和HostName不变，基于Headless Service（即没有 Cluster IP的Service）来实现

- 有序部署，有序扩展，即Pod是有顺序的，在部署或者扩展的时候要依据定义的顺序依次依次进行（即从0到 N-1，在下一个Pod运行之前所有之前的Pod必须都是Running和Ready状态），基于init containers来实 现

- 有序收缩，有序删除（即从N-1到0）

1.2.7 Horizontal Pod Autoscaling

应用的资源使用率通常都有高峰和低谷的时候，如何削峰填谷，提高集群的整体资源利用率，让service中的Pod 个数自动调整呢？这就有赖于Horizontal Pod Autoscaling了，顾名思义，使Pod水平自动缩放

### 2,kubernetes Deployment控制器

2.1 RS 与 RC 与 Deployment 关联

RC （ReplicationController ）主要的作用就是用来确保容器应用的副本数始终保持在用户定义的副本数 。即如 果有容器异常退出，会自动创建新的Pod来替代；而如果异常多出来的容器也会自动回收； Kubernetes 官方建议使用 RS（ReplicaSet ） 替代 RC （ReplicationController ） 进行部署，RS 跟 RC 没有 本质的不同，只是名字不一样，并且 RS 支持集合式的 selector.

```yaml
apiVersion: extensions/v1beta1
kind: ReplicaSet
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      labels:
        tier: frontend
    spec:
      containers:
      - name: myapp
        image: wangyanglinux/myapp:v1 
        env:
        - name: GET_HOSTS_FROM
          value: dns
        ports:
        - containerPort: 80
```

2.2 RS 与 Deployment 的关联

 ![img](https://img-blog.csdnimg.cn/20200611195902200.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDgwMDkxNQ==,size_16,color_FFFFFF,t_70) 

2.3 Deployment

Deployment 为 Pod 和 ReplicaSet 提供了一个声明式定义(declarative)方法，用来替代以前的 ReplicationController 来方便的管理应用。典型的应用场景包括：

- 定义Deployment来创建Pod和ReplicaSet

- 滚动升级和回滚应用

- 扩容和缩容

- 暂停和继续Deployment

2.3.1 部署一个简单的 Nginx 应用

```yaml
 apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: nginx-deployment 
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx 
        image: nginx:1.7.9 
        ports:
        - containerPort: 80
```

- 查看pod的标签

```
[root@k8s-master01 ymlfile]# kubectl get pod --show-labels
```

- 修改pod标签

```shell
[root@k8s-master01 ~]# kubectl get pods --show-labels
frontend-hs79r   1/1     Running   0          8m26s   tier=frontend
frontend-wh5hs   1/1     Running   0          27s     tier=frontend
frontend-zrzft   1/1     Running   0          8m26s   tier=frontend
 
[root@k8s-master01 ~]# kubectl lable pod frontend-fnzt5 tier=frontend --overwrite=True
[root@k8s-master01 ~]# kubectl get pods --show-labels
NAME             READY   STATUS    RESTARTS   AGE     LABELS
frontend-fnzt5   1/1     Running   0          8m26s   tier=frontend1
frontend-hs79r   1/1     Running   0          8m26s   tier=frontend
frontend-wh5hs   1/1     Running   0          27s     tier=frontend
frontend-zrzft   1/1     Running   0          8m26s   tier=frontend
```

```shell
kubectl create -f https://kubernetes.io/docs/user-guide/nginx-deployment.yaml --record 
## --record参数可以记录命令，我们可以很方便的查看每次 revision 的变化
```

2.3.2 扩容

```
kubectl scale deployment nginx-deployment --replicas 10
```


2.3.3 如果集群支持 horizontal pod autoscaling 的话，还可以为Deployment设置自动扩展

```
kubectl autoscale deployment nginx-deployment --min=10 --max=15 --cpu-percent=80
```


2.3.4 更新镜像也比较简单

```
kubectl set image deployment/nginx-deployment nginx=nginx:1.9.1
```


2.3.5 回滚

```
kubectl rollout undo deployment/nginx-deployment
```


2.4 更新 Deployment

假如我们现在想要让 nginx pod 使用 nginx:1.9.1 的镜像来代替原来的 nginx:1.7.9 的镜像:

```
$ kubectl set image deployment/nginx-deployment nginx=nginx:1.9.1 
deployment "nginx-deployment" image updated
```


可以使用 edit 命令来编辑 Deployment:

```
$ kubectl edit deployment/nginx-deployment deployment "nginx-deployment" edited
```


查看 rollout 的状态:

```
$ kubectl rollout status deployment/nginx-deployment
Waiting for rollout to finish: 2 out of 3 new replicas have been updated... deployment "nginx-deployment" successfully rolled out
```


查看历史 RS:

```
$ kubectl get rs
NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-1564180365   3         3         0       6s
nginx-deployment-2035384211   0         0         0       36s
```


2.5 Deployment 更新策略

- Deployment 可以保证在升级时只有一定数量的 Pod 是 down 的。默认的，它会确保至少有比期望的Pod数量少 一个是up状态（最多一个不可用）

- Deployment 同时也可以确保只创建出超过期望数量的一定数量的 Pod。默认的，它会确保最多比期望的Pod数 量多一个的 Pod 是 up 的（最多1个 surge ） 未来的 Kuberentets 版本中，将从1-1变成25%-25%(新建1个新的，删除一个老的)

```
$ kubectl describe deployments
```

2.6 Rollover（多个rollout并行)

假如您创建了一个有5个 niginx:1.7.9  replica的 Deployment，但是当还只有3个 nginx:1.7.9 的 replica 创建 出来的时候您就开始更新含有5个 nginx:1.9.1  replica 的 Deployment。在这种情况下，Deployment 会立即 杀掉已创建的3个 nginx:1.7.9 的 Pod，并开始创建 nginx:1.9.1 的 Pod。它不会等到所有的5个 nginx:1.7.9 的 Pod 都创建完成后才开始改变航道

2.7 回退 Deployment

```
kubectl set image deployment/nginx-deployment nginx=nginx:1.91
kubectl rollout status deployments nginx-deployment
kubectl get pods
kubectl rollout history deployment/nginx-deployment
kubectl rollout undo deployment/nginx-deployment
kubectl rollout undo deployment/nginx-deployment --to-revision=2   ## 可以使用 --revision参数指定 某个历史版本
kubectl rollout pause deployment/nginx-deployment    ## 暂停 deployment 的更新
```

您可以用 kubectl rollout status 命令查看 Deployment 是否完成。如果 rollout 成功完成， kubectl rollout status 将返回一个0值的 Exit Code

```
$ kubectl rollout status deploy/nginx
Waiting for rollout to finish: 2 of 3 updated replicas are available... 
deployment "nginx" successfully rolled out
$ echo $?
0
```

2.8 清理 Policy

可以通过设置 .spec.revisonHistoryLimit 项来指定 deployment 最多保留多少 revision 历史记录。默认的会 保留所有的 revision；如果将该项设置为0，Deployment 就不允许回退了
3. ### kubernetes DaemonSet

  3.1 什么是 DaemonSet

DaemonSet 确保全部（或者一些）Node 上运行一个 Pod 的副本。当有 Node 加入集群时，也会为他们新增一 个 Pod 。当有 Node 从集群移除时，这些 Pod 也会被回收。删除 DaemonSet 将会删除它创建的所有 Pod

使用 DaemonSet 的一些典型用法：

- 运行集群存储 daemon，例如在每个 Node 上运行 glusterd 、 ceph
- 在每个 Node 上运行日志收集 daemon，例如 fluentd 、 logstash
- 在每个 Node 上运行监控 daemon，例如 Prometheus Node Exporter、 collectd 、Datadog 代理、 New Relic 代理，或 Ganglia gmond

```
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: deamonset-example
  labels:
    app: daemonset
spec:
  selector:
    matchLabels:
      name: deamonset-example 
  template:
    metadata:
      labels:
        name: deamonset-example 
    spec:
      containers:
      - name: daemonset-example 
        image: wangyanglinux/myapp:v1
```

### 4, kubernetes Job

 Job 负责批处理任务，即仅执行一次的任务，它保证批处理任务的一个或多个 Pod 成功结束 



特殊说明：

- spec.template格式同Pod

- RestartPolicy仅支持Never或OnFailure

- 单个Pod时，默认Pod成功运行后Job即结束

- .spec.completions 标志Job结束需要成功运行的Pod个数，默认为1

- .spec.parallelism 标志并行运行的Pod的个数，默认为1

- spec.activeDeadlineSeconds 标志失败Pod的重试最大时间，超过这个时间不会继续重试

 Example 

```
apiVersion: batch/v1
kind: Job
metadata:
  name: pi
spec:
  template:
    metadata:
      name: pi
    spec:
      containers:
      - name: pi
        image: perl
        command: ["perl",  "-Mbignum=bpi", "-wle", "print bpi(2000)"] 
      restartPolicy: Never
```

4.1 CronJob Spec

- spec.template格式同Pod

- RestartPolicy仅支持Never或OnFailure

- 单个Pod时，默认Pod成功运行后Job即结束

- .spec.completions 标志Job结束需要成功运行的Pod个数，默认为1

- .spec.parallelism 标志并行运行的Pod的个数，默认为1

- spec.activeDeadlineSeconds 标志失败Pod的重试最大时间，超过这个时间不会继续重试

4.2 CronJob

Cron Job 管理基于时间的 Job，即：

- 在给定时间点只运行一次
- 周期性地在给定时间点运行 使用条件：当前使用的 Kubernetes 集群，版本 >= 1.8（对 CronJob）

 典型的用法如下所示： 

- 在给定的时间点调度 Job 运行
- 创建周期性运行的 Job，例如：数据库备份、发送邮件

 4.3 CronJob Spec 

- .spec.schedule ：调度，必需字段，指定任务运行周期，格式同 Cron

- .spec.jobTemplate ：Job 模板，必需字段，指定需要运行的任务，格式同 Job - - .spec.startingDeadlineSeconds ：启动 Job 的期限（秒级别），该字段是可选的。如果因为任何原因而错 过了被调度的时间，那么错过执行时间的 Job 将被认为是失败的。如果没有指定，则没有期限

- .spec.concurrencyPolicy ：并发策略，该字段也是可选的。它指定了如何处理被 Cron Job 创建的 Job 的 并发执行。只允许指定下面策略中的一种：
  - Allow （默认）：允许并发运行 Job
  - Forbid ：禁止并发运行，如果前一个还没有完成，则直接跳过下一个
  - Replace ：取消当前正在运行的 Job，用一个新的来替换 注意，当前策略只能应用于同一个 Cron Job 创建的 Job。如果存在多个 Cron Job，它们创建的 Job 之间总 是允许并发运行。

- .spec.suspend ：挂起，该字段也是可选的。如果设置为 true ，后续所有执行都会被挂起。它对已经开始 执行的 Job 不起作用。默认值为 false 。

- .spec.successfulJobsHistoryLimit 和 .spec.failedJobsHistoryLimit ：历史限制，是可选的字段。它 们指定了可以保留多少完成和失败的 Job。默认情况下，它们分别设置为 3 和 1 。设置限制的值为 0 ，相 关类型的 Job 完成后将不会被保留。

 Example: 

```
 apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: hello
spec:
  schedule: "*/1 * * * *" 
  jobTemplate:
    spec:
      template: 
        spec:
          containers:
          - name: hello
            image: busybox
            args:
            - /bin/sh
            - -c
            - date; echo Hello from the Kubernetes cluster 
          restartPolicy: OnFailure
```

```
$ kubectl get cronjob
NAME      SCHEDULE      SUSPEND   ACTIVE    LAST-SCHEDULE
hello     */1 * * * *   False     0         <none>
$ kubectl get jobs
NAME               DESIRED   SUCCESSFUL   AGE
hello-1202039034   1         1            49s
$ pods=$(kubectl get pods --selector=job-name=hello-1202039034 --output=jsonpath= {.items..metadata.name})
$ kubectl logs $pods
Mon Aug 29 21:34:09 UTC 2016
Hello from the Kubernetes cluster
 
# 注意，删除 cronjob 的时候不会自动删除 job，这些 job 可以用 kubectl delete job 来删除 
$ kubectl delete cronjob hello
cronjob "hello" deleted
```

 4.4 CrondJob 本身的一些限制 

```
创建 Job 操作应该是幂等的
```


### rs

```yaml
apiVersion: extensions/v1beta1
kind: ReplicaSet
metadata:
  name: frontend
spec:
  replicas: 2
  selector:    ##选择标签
    matchLabels:    #匹配标签
      app: frontend   #标签名
  template:    #模板
    metadata:
      labels:
        app: frontend
    spec:
      containers:
      - name: nginx
        image: nginx
        env:   #注入环境变量
        - name: GET_HOST_FROM
          value: dns
        ports:
        - containerPort: 80
```

```yaml
[root@master01 myapp]# vim frontend.yaml
[root@master01 myapp]# kubectl get pod
NAME                    READY   STATUS    RESTARTS   AGE
myapp-pod               1/1     Running   0          23h
readiness-httpget-pod   1/1     Running   0          22h
[root@master01 myapp]# kubectl delete pod --all
pod "myapp-pod" deleted
pod "readiness-httpget-pod" deleted
[root@master01 myapp]# kubectl get pod
No resources found.
[root@master01 myapp]# kubectl get svc
NAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1        <none>        443/TCP   33h
mydb         ClusterIP   10.96.201.14     <none>        80/TCP    22h
myservice    ClusterIP   10.110.163.186   <none>        80/TCP    22h
[root@master01 myapp]# kubectl delete svc mydb myservice
service "mydb" deleted
service "myservice" deleted
[root@master01 myapp]# kubectl get svc
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   33h
[root@master01 myapp]# kubectl get deployment
No resources found.
# 上面为清空环境
[root@master01 myapp]# kubectl create -f  frontend.yaml 
replicaset.extensions/frontend created
# 启动后会创建两个pod，并由rs控制器维持在两个pod，删除了会重新创建
[root@master01 myapp]# kubectl get pod
NAME             READY   STATUS              RESTARTS   AGE
frontend-97bmm   0/1     ContainerCreating   0          10s
frontend-h2zkk   0/1     ContainerCreating   0          10s
# 删除会维持为两个副本，重新创建两个pod
[root@master01 myapp]# kubectl delete pod --all
pod "frontend-97bmm" deleted
pod "frontend-h2zkk" deleted
[root@master01 myapp]# kubectl get pod
NAME             READY   STATUS    RESTARTS   AGE
frontend-b94sl   1/1     Running   0          20s
frontend-ft92m   1/1     Running   0          20s
# 查询pod和pod的labels
[root@master01 myapp]# kubectl get pod --show-labels
NAME             READY   STATUS    RESTARTS   AGE   LABELS
frontend-b94sl   1/1     Running   0          57s   app=frontend
frontend-ft92m   1/1     Running   0          57s   app=frontend
# 更改pod的label
[root@master01 myapp]# kubectl label pod frontend-b94sl app=frontend1 --overwrite
pod/frontend-b94sl labeled
# 会发现标签改变了，会新起一个pod
[root@master01 myapp]# kubectl get pod --show-labels
NAME             READY   STATUS    RESTARTS   AGE     LABELS
frontend-b94sl   1/1     Running   0          2m16s   app=frontend1
frontend-ft92m   1/1     Running   0          2m16s   app=frontend
frontend-rmr9j   1/1     Running   0          16s     app=frontend
# 删除rs 不会的pod app=frountend1 对应的pod，这里删错了   
[root@master01 myapp]# kubectl delete pod --all
pod "frontend-b94sl" deleted
pod "frontend-ft92m" deleted
pod "frontend-rmr9j" deleted
[root@master01 myapp]# kubectl get pod --show-labels
NAME             READY   STATUS              RESTARTS   AGE   LABELS
frontend-jf8ql   0/1     ContainerCreating   0          14s   app=frontend
frontend-n7rpz   0/1     ContainerCreating   0          14s   app=frontend
[root@master01 myapp]# kubectl get rs
NAME       DESIRED   CURRENT   READY   AGE
frontend   2         2         2       7m9s

```

### deployment

```
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: hub.st.com/library/mynginx:v1
        ports:
        - containerPort: 80
```

扩容

```
kubectl scale deployment nginx-deployment --replicas 10
```

如果集群支持 horizontal pod autoscaling 的话，还可以为Deployment设置自动扩展：

```
kubectl autoscale deployment nginx-deployment --min=10 --max=15 --cpu-percent=80
```

更新镜像也比较简单:

```
kubectl set image deployment/nginx-deployment nginx=nginx:1.9.1
```

回滚：

```shell
kubectl rollout undo deployment/nginx-deployment

# 回滚到指定版本
kubectl rollout undo deployment/nginx-deployment --to-revision=3
```

回滚查看状态

```shell
kubectl rollout status deployment/nginx-deployment

# 也可以通过$?去查询，返回为0表示成功
[root@master01 myapp]# echo $?
0

```

### DaemonSet

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: deamonset-example
  labels:
    app: daemonset
spec:
  selector:
    matchLabels:
      name: deamonset-example
  template:
    metadata:
      labels:
        name: deamonset-example
    spec:
      containers:
      - name: daemonset-example
        image: hub.st.com/mynginx:v1
```

### job

#### cronjob Spec

- spec.template格式同Pod
- RestartPolicy仅支持Never或OnFailure
- 单个Pod时，默认Pod成功运行后Job即结束
- `.spec.completions`标志Job结束需要成功运行的Pod个数，默认为1
- `.spec.parallelism`标志并行运行的Pod的个数，默认为1
- `spec.activeDeadlineSeconds`标志失败Pod的重试最大时间，超过这个时间

#### Bare Pods

所谓Bare Pods是指直接用PodSpec来创建的Pod（即不在ReplicaSets或者ReplicationController的管理之下的Pods）。这些Pod在Node重启后不会自动重启，但Job则会创建新的Pod继续任务。所以，推荐使用Job来替代Bare Pods，即便是应用只需要一个Pod。

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: pi
spec:
  template:
    metadata:
      name: pi
    spec:
      containers:
      - name: pi
        image: perl
        command: ["perl",  "-Mbignum=bpi", "-wle", "print bpi(2000)"]
      restartPolicy: Never
```

```yaml
[root@master01 myapp]# kubectl get pod -o wide
NAME                      READY   STATUS      RESTARTS   AGE     IP            NODE     NOMINATED NODE   READINESS GATES
deamonset-example-l9tkp   1/1     Running     0          6m56s   10.244.1.28   node01   <none>           <none>
deamonset-example-zlmrp   1/1     Running     0          7m29s   10.244.2.29   node02   <none>           <none>
# 表示计算完成了
pi-qw8qk                  0/1     Completed   0          4m49s   10.244.1.29   node01   <none>           <none>
[root@master01 myapp]# kubectl get job
NAME   COMPLETIONS   DURATION   AGE
pi     1/1           52s        8m22s
[root@master01 myapp]# kubectl logs pi-qw8qk
3.1415926535897932384626433832795028841971693993751058209749445923078164062862089986280348253421170679821480865132823066470938446095505822317253594081284811174502841027019385211055596446229489549303819644288109756659334461284756482337867831652712019091456485669234603486104543266482133936072602491412737245870066063155881748815209209628292540917153643678925903600113305305488204665213841469519415116094330572703657595919530921861173819326117931051185480744623799627495673518857527248912279381830119491298336733624406566430860213949463952247371907021798609437027705392171762931767523846748184676694051320005681271452635608277857713427577896091736371787214684409012249534301465495853710507922796892589235420199561121290219608640344181598136297747713099605187072113499999983729780499510597317328160963185950244594553469083026425223082533446850352619311881710100031378387528865875332083814206171776691473035982534904287554687311595628638823537875937519577818577805321712268066130019278766111959092164201989380952572010654858632788659361533818279682303019520353018529689957736225994138912497217752834791315155748572424541506959508295331168617278558890750983817546374649393192550604009277016711390098488240128583616035637076601047101819429555961989467678374494482553797747268471040475346462080466842590694912933136770289891521047521620569660240580381501935112533824300355876402474964732639141992726042699227967823547816360093417216412199245863150302861829745557067498385054945885869269956909272107975093029553211653449872027559602364806654991198818347977535663698074265425278625518184175746728909777727938000816470600161452491921732172147723501414419735685481613611573525521334757418494684385233239073941433345477624168625189835694855620992192221842725502542568876717904946016534668049886272327917860857843838279679766814541009538837863609506800642251252051173929848960841284886269456042419652850222106611863067442786220391949450471237137869609563643719172874677646575739624138908658326459958133904780275901
```

### cron job

#### CronJob Spec

- `.spec.schedule`：**调度**，必需字段，指定任务运行周期，格式同 [Cron](https://en.wikipedia.org/wiki/Cron)

- `.spec.jobTemplate`：**Job 模板**，必需字段，指定需要运行的任务，格式同 [Job](https://www.bookstack.cn/read/kubernetes-handbook-201910/concepts-job.md)

- `.spec.startingDeadlineSeconds` ：**启动 Job 的期限（秒级别）**，该字段是可选的。如果因为任何原因而错过了被调度的时间，那么错过执行时间的 Job 将被认为是失败的。如果没有指定，则没有期限

- `.spec.concurrencyPolicy`：**并发策略**，该字段也是可选的。它指定了如何处理被 Cron Job 创建的 Job 的并发执行。只允许指定下面策略中的一种：

  - `Allow`（默认）：允许并发运行 Job
  - `Forbid`：禁止并发运行，如果前一个还没有完成，则直接跳过下一个
  - `Replace`：取消当前正在运行的 Job，用一个新的来替换

  注意，当前策略只能应用于同一个 Cron Job 创建的 Job。如果存在多个 Cron Job，它们创建的 Job 之间总是允许并发运行。

- `.spec.suspend` ：**挂起**，该字段也是可选的。如果设置为 `true`，后续所有执行都会被挂起。它对已经开始执行的 Job 不起作用。默认值为 `false`。

- `.spec.successfulJobsHistoryLimit` 和 `.spec.failedJobsHistoryLimit` ：**历史限制**，是可选的字段。它们指定了可以保留多少完成和失败的 Job。

  默认情况下，它们分别设置为 `3` 和 `1`。设置限制的值为 `0`，相关类

```yaml
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: hello
spec:
  schedule: "*/1 * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: hello
            image: busybox
            args:
            - /bin/sh
            - -c
            - date; echo Hello from the Kubernetes cluster
          restartPolicy: OnFailure
```

```shell
[root@master01 myapp]# kubectl get job
NAME               COMPLETIONS   DURATION   AGE
hello-1594828980   1/1           6s         2m25s
hello-1594829040   1/1           17s        85s
hello-1594829100   1/1           16s        25s
[root@master01 myapp]# kubectl get pod -w
NAME                     READY   STATUS      RESTARTS   AGE
hello-1594828980-xv5bj   0/1     Completed   0          2m34s
hello-1594829040-jx67j   0/1     Completed   0          94s
hello-1594829100-zzcz4   0/1     Completed   0          34s
^C[root@master01 myapp]# kubectl log hello-1594828980-xv5bj
log is DEPRECATED and will be removed in a future version. Use logs instead.
Wed Jul 15 16:03:14 UTC 2020
Hello from the Kubernetes cluster

```

### service

暴露**pod**的访问方式，比如一个**deployment** 的 **replicas: 3** 能统一由**service**负载均衡访问

​	myapp-service.yaml 

```yaml
apiVersion: v1
kind: Service
metadata:
  name: myapp
  namespace: default
spec:
  type: ClusterIP
  selector:
    app: myapp
    release: stabel
  ports:
  - name: http
    port: 80
    targetPort: 80
```



```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deploy
  namespace: default
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp
      release: stabel
  template:
    metadata:
      labels:
        app: myapp
        release: stabel
        env: test
    spec:
      containers:
      - name: myapp
        image: wangyanglinux/myapp:v2
        imagePullPolicy: IfNotPresent
        ports:
        - name: http
          containerPort: 80
```

svc-deployment.yaml 

```yaml
# 创建service
[root@master01 myapp]# kubectl apply -f myapp-service.yaml 
service/myapp created
[root@master01 myapp]# kubectl get svc
NAME         TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1     <none>        443/TCP   4d9h
myapp        ClusterIP   10.109.16.1   <none>        80/TCP    24s
# 创建deployment
[root@master01 myapp]# kubectl apply -f svc-deployment.yaml 
deployment.apps/myapp-deploy created
# 查看pod
[root@master01 myapp]# kubectl get pod -o wide
NAME                            READY   STATUS    RESTARTS   AGE   IP            NODE     NOMINATED NODE   READINESS GATES
myapp-deploy-6cc7c66999-dpncj   1/1     Running   0          31s   10.244.1.39   node01   <none>           <none>
myapp-deploy-6cc7c66999-h642r   1/1     Running   0          31s   10.244.2.38   node02   <none>           <none>
myapp-deploy-6cc7c66999-qqdm6   1/1     Running   0          31s   10.244.1.38   node01   <none>           <none>
# 查看service
[root@master01 myapp]# kubectl get svc -o wide
NAME         TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE    SELECTOR
kubernetes   ClusterIP   10.96.0.1     <none>        443/TCP   4d9h   <none>
myapp        ClusterIP   10.109.16.1   <none>        80/TCP    98s    app=myapp,release=stabel
[root@master01 myapp]# curl 10.109.16.1
Hello MyApp | Version: v2 | <a href="hostname.html">Pod Name</a>
# 查看ipvs
[root@master01 myapp]# ipvsadm -Ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
  -> RemoteAddress:Port           Forward Weight ActiveConn InActConn
TCP  10.96.0.1:443 rr
  -> 192.168.1.102:6443           Masq    1      3          0         
TCP  10.96.0.10:53 rr
  -> 10.244.0.4:53                Masq    1      0          0         
  -> 10.244.0.5:53                Masq    1      0          0         
TCP  10.96.0.10:9153 rr
  -> 10.244.0.4:9153              Masq    1      0          0         
  -> 10.244.0.5:9153              Masq    1      0          0         
# service和pod的隐射关系
TCP  10.109.16.1:80 rr
  -> 10.244.1.38:80               Masq    1      0          0         
  -> 10.244.1.39:80               Masq    1      0          0         
  -> 10.244.2.38:80               Masq    1      0          0         
UDP  10.96.0.10:53 rr
  -> 10.244.0.4:53                Masq    1      0          0         
  -> 10.244.0.5:53                Masq    1      0          0         


```

#### headless

**可以通过域名访问**

svc-headless.yaml

```yaml
apiVersion: v1
kind: Service
metadata:
  name: myapp-headless
  namespace: default
spec:
  selector:
    app: myapp
  clusterIP: "None"
  ports:
  - port: 80
    targetPort: 80

```

```shell
[root@master01 myapp]# vim svc-headless.yaml
[root@master01 myapp]# kubectl apply -f svc-headless.yaml 
service/myapp-headless created
[root@master01 myapp]# kubectl get svc
NAME             TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE
kubernetes       ClusterIP   10.96.0.1     <none>        443/TCP   4d9h
myapp            ClusterIP   10.109.16.1   <none>        80/TCP    10m
myapp-headless   ClusterIP   None          <none>        80/TCP    15s
# 查看coredns服务的host文件
[root@master01 myapp]# kubectl get pod -n kube-system -o wide
NAME                               READY   STATUS    RESTARTS   AGE    IP              NODE       NOMINATED NODE   READINESS GATES
# 两个dns，随便进一个
coredns-6967fb4995-bcvzs           1/1     Running   1          4d9h   10.244.0.4      master01   <none>           <none>
coredns-6967fb4995-csl6t           1/1     Running   1          4d9h   10.244.0.5      master01   <none>           <none>
etcd-master01                      1/1     Running   2          4d9h   192.168.1.102   master01   <none>           <none>
kube-apiserver-master01            1/1     Running   2          4d9h   192.168.1.102   master01   <none>           <none>
kube-controller-manager-master01   1/1     Running   3          4d9h   192.168.1.102   master01   <none>           <none>
kube-flannel-ds-amd64-4mqhf        1/1     Running   2          4d8h   192.168.1.103   node01     <none>           <none>
kube-flannel-ds-amd64-fbswf        1/1     Running   1          4d9h   192.168.1.102   master01   <none>           <none>
kube-flannel-ds-amd64-vq5x2        1/1     Running   1          4d8h   192.168.1.104   node02     <none>           <none>
kube-proxy-6v9bg                   1/1     Running   1          4d8h   192.168.1.104   node02     <none>           <none>
kube-proxy-jrtc7                   1/1     Running   2          4d9h   192.168.1.102   master01   <none>           <none>
kube-proxy-n4ddv                   1/1     Running   2          4d8h   192.168.1.103   node01     <none>           <none>
kube-scheduler-master01            1/1     Running   2          4d9h   192.168.1.102   master01   <none>           <none>
# 解析dns域名，需要dig命令，安装bind-utils
[root@master01 myapp]# yum -y install bind-utils
Loaded plugins: fastestmirror
Loading mirror speeds from cached hostfile
 * base: mirrors.163.com
 * elrepo: mirrors.tuna.tsinghua.edu.cn
 * extras: mirrors.aliyun.com
 * updates: mirrors.aliyun.com
base                                                                                | 3.6 kB  00:00:00     
docker-ce-stable                                                                    | 3.5 kB  00:00:00     
elrepo                                                                              | 2.9 kB  00:00:00     
extras                                                                              | 2.9 kB  00:00:00     
kubernetes                                                                          | 1.4 kB  00:00:00     
updates                                                                             | 2.9 kB  00:00:00     
(1/2): updates/7/x86_64/primary_db                                                  | 3.0 MB  00:00:00     
(2/2): elrepo/primary_db                                                            | 369 kB  00:00:00     
Resolving Dependencies
--> Running transaction check
---> Package bind-utils.x86_64 32:9.11.4-16.P2.el7_8.6 will be installed
--> Processing Dependency: bind-libs(x86-64) = 32:9.11.4-16.P2.el7_8.6 for package: 32:bind-utils-9.11.4-16.P2.el7_8.6.x86_64
--> Processing Dependency: liblwres.so.160()(64bit) for package: 32:bind-utils-9.11.4-16.P2.el7_8.6.x86_64
--> Processing Dependency: libbind9.so.160()(64bit) for package: 32:bind-utils-9.11.4-16.P2.el7_8.6.x86_64
--> Running transaction check
---> Package bind-libs.x86_64 32:9.11.4-16.P2.el7_8.6 will be installed
--> Finished Dependency Resolution

Dependencies Resolved

===========================================================================================================
 Package                Arch               Version                               Repository           Size
===========================================================================================================
Installing:
 bind-utils             x86_64             32:9.11.4-16.P2.el7_8.6               updates             259 k
Installing for dependencies:
 bind-libs              x86_64             32:9.11.4-16.P2.el7_8.6               updates             156 k

Transaction Summary
===========================================================================================================
Install  1 Package (+1 Dependent package)

Total download size: 415 k
Installed size: 769 k
Downloading packages:
(1/2): bind-libs-9.11.4-16.P2.el7_8.6.x86_64.rpm                                    | 156 kB  00:00:00     
(2/2): bind-utils-9.11.4-16.P2.el7_8.6.x86_64.rpm                                   | 259 kB  00:00:00     
-----------------------------------------------------------------------------------------------------------
Total                                                                      1.7 MB/s | 415 kB  00:00:00     
Running transaction check
Running transaction test
Transaction test succeeded
Running transaction
  Installing : 32:bind-libs-9.11.4-16.P2.el7_8.6.x86_64                                                1/2 
  Installing : 32:bind-utils-9.11.4-16.P2.el7_8.6.x86_64                                               2/2 
  Verifying  : 32:bind-libs-9.11.4-16.P2.el7_8.6.x86_64                                                1/2 
  Verifying  : 32:bind-utils-9.11.4-16.P2.el7_8.6.x86_64                                               2/2 

Installed:
  bind-utils.x86_64 32:9.11.4-16.P2.el7_8.6                                                                

Dependency Installed:
  bind-libs.x86_64 32:9.11.4-16.P2.el7_8.6                                                                 

Complete!
[root@master01 myapp]# kubectl get pod -n kube-system -o wide
NAME                               READY   STATUS    RESTARTS   AGE    IP              NODE       NOMINATED NODE   READINESS GATES
coredns-6967fb4995-bcvzs           1/1     Running   1          4d9h   10.244.0.4      master01   <none>           <none>
coredns-6967fb4995-csl6t           1/1     Running   1          4d9h   10.244.0.5      master01   <none>           <none>
etcd-master01                      1/1     Running   2          4d9h   192.168.1.102   master01   <none>           <none>
kube-apiserver-master01            1/1     Running   2          4d9h   192.168.1.102   master01   <none>           <none>
kube-controller-manager-master01   1/1     Running   3          4d9h   192.168.1.102   master01   <none>           <none>
kube-flannel-ds-amd64-4mqhf        1/1     Running   2          4d8h   192.168.1.103   node01     <none>           <none>
kube-flannel-ds-amd64-fbswf        1/1     Running   1          4d9h   192.168.1.102   master01   <none>           <none>
kube-flannel-ds-amd64-vq5x2        1/1     Running   1          4d8h   192.168.1.104   node02     <none>           <none>
kube-proxy-6v9bg                   1/1     Running   1          4d8h   192.168.1.104   node02     <none>           <none>
kube-proxy-jrtc7                   1/1     Running   2          4d9h   192.168.1.102   master01   <none>           <none>
kube-proxy-n4ddv                   1/1     Running   2          4d8h   192.168.1.103   node01     <none>           <none>
kube-scheduler-master01            1/1     Running   2          4d9h   192.168.1.102   master01   <none>           <none>
# 解析无头service的域名
# [headless service名][命名空间名][集群地址] @[coredns 地址]
[root@master01 myapp]# dig -t A myapp-headless.default.svc.cluster.local. @10.244.0.4

; <<>> DiG 9.11.4-P2-RedHat-9.11.4-16.P2.el7_8.6 <<>> -t A myapp-headless.default.svc.cluster.local. @10.244.0.4
;; global options: +cmd
;; Got answer:
;; WARNING: .local is reserved for Multicast DNS
;; You are currently testing what happens when an mDNS query is leaked to DNS
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 57965
;; flags: qr aa rd; QUERY: 1, ANSWER: 3, AUTHORITY: 0, ADDITIONAL: 1
;; WARNING: recursion requested but not available

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4096
;; QUESTION SECTION:
;myapp-headless.default.svc.cluster.local. IN A

;; ANSWER SECTION:
# 这里能看到三条对应关系
myapp-headless.default.svc.cluster.local. 30 IN	A 10.244.1.39
myapp-headless.default.svc.cluster.local. 30 IN	A 10.244.1.38
myapp-headless.default.svc.cluster.local. 30 IN	A 10.244.2.38

;; Query time: 209 msec
;; SERVER: 10.244.0.4#53(10.244.0.4)
;; WHEN: Thu Jul 16 22:24:27 CST 2020
;; MSG SIZE  rcvd: 237

[root@master01 myapp]# kubectl get pod -o wide
NAME                            READY   STATUS    RESTARTS   AGE   IP            NODE     NOMINATED NODE   READINESS GATES
myapp-deploy-6cc7c66999-dpncj   1/1     Running   0          15m   10.244.1.39   node01   <none>           <none>
myapp-deploy-6cc7c66999-h642r   1/1     Running   0          15m   10.244.2.38   node02   <none>           <none>
myapp-deploy-6cc7c66999-qqdm6   1/1     Running   0          15m   10.244.1.38   node01   <none>           <none>


```

#### NodePort

在任意节点暴露端口，访问

svcNodePod.yaml

```yaml
apiVersion: v1
kind: Service
metadata:
  name: myapp
  namespace: default
spec:
  type: NodePort
  selector:
    app: myapp
    release: stabel
  ports:
  - name: http
    port: 80
    targetPort: 80
```



```shell
[root@master01 myapp]# vim svcNodePod.yaml
[root@master01 myapp]# kubectl apply -f svcNodePod.yaml 
service/myapp configured
[root@master01 myapp]# kubectl get svc
NAME             TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)        AGE
kubernetes       ClusterIP   10.96.0.1     <none>        443/TCP        4d9h
myapp            NodePort    10.109.16.1   <none>        80:32349/TCP   18m
myapp-headless   ClusterIP   None          <none>        80/TCP         8m13s
# 可以通过任意节点的ip地址+32349访问（注意不是NodePort service host，是node的host）
[root@master01 myapp]# kubectl get pod
NAME                            READY   STATUS    RESTARTS   AGE
myapp-deploy-6cc7c66999-dpncj   1/1     Running   0          20m
myapp-deploy-6cc7c66999-h642r   1/1     Running   0          20m
myapp-deploy-6cc7c66999-qqdm6   1/1     Running   0          20m
[root@master01 myapp]# kubectl get pod |grep yaml
[root@master01 myapp]# kubectl describe svc myapp
Name:                     myapp
Namespace:                default
Labels:                   <none>
Annotations:              kubectl.kubernetes.io/last-applied-configuration:
                            {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"myapp","namespace":"default"},"spec":{"ports":[{"name":"http","po...
Selector:                 app=myapp,release=stabel
Type:                     NodePort
IP:                       10.109.16.1
Port:                     http  80/TCP
TargetPort:               80/TCP
NodePort:                 http  32349/TCP
Endpoints:                10.244.1.38:80,10.244.1.39:80,10.244.2.38:80
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   <none>
[root@master01 myapp]# netstat -anpt |grep :32349
tcp6       1      0 :::32349                :::*                    LISTEN      87126/kube-proxy    
tcp6      82      0 10.109.16.1:32349       192.168.1.102:52418     CLOSE_WAIT  -         

```

#### externalName

当访问**my-service**能对应到**my.database.example.com**

externalName.yaml

```yaml
kind: Service
apiVersion: v1
metadata:
  name: my-service
  namespace: default
spec:
  type: ExternalName
  externalName: my.database.example.com
  ports:
  - port: 12345

```



```shell
[root@master01 myapp]# kubectl apply -f externalName.yaml 
service/my-service created
[root@master01 myapp]# kubectl get svc
NAME             TYPE           CLUSTER-IP    EXTERNAL-IP               PORT(S)        AGE
kubernetes       ClusterIP      10.96.0.1     <none>                    443/TCP        4d10h
my-service       ExternalName   <none>        my.database.example.com   12345/TCP      9s
myapp            NodePort       10.109.16.1   <none>                    80:32349/TCP   37m
myapp-headless   ClusterIP      None          <none>                    80/TCP         26m

# 能看到 当访问my-service 能对应到my.database.example.com
[root@master01 myapp]# dig -t A my-service.default.svc.cluster.local. @10.244.0.4

; <<>> DiG 9.11.4-P2-RedHat-9.11.4-16.P2.el7_8.6 <<>> -t A my-service.default.svc.cluster.local. @10.244.0.4
;; global options: +cmd
;; Got answer:
;; WARNING: .local is reserved for Multicast DNS
;; You are currently testing what happens when an mDNS query is leaked to DNS
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 65275
;; flags: qr aa rd; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1
;; WARNING: recursion requested but not available

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4096
;; QUESTION SECTION:
;my-service.default.svc.cluster.local. IN A

;; ANSWER SECTION:
my-service.default.svc.cluster.local. 30 IN CNAME my.database.example.com.

;; Query time: 343 msec
;; SERVER: 10.244.0.4#53(10.244.0.4)
;; WHEN: Thu Jul 16 22:47:34 CST 2020
;; MSG SIZE  rcvd: 138
```

### Ingress

##### ingress-nginx安装

```shell
wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.25.0/deploy/static/mandatory.yaml

# 启动pod
apply -f mandatory.yaml
```

mandatory.yaml 

修改授权和镜像地址

```yaml


---

kind: ConfigMap
apiVersion: v1
metadata:
  name: nginx-configuration
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---
kind: ConfigMap
apiVersion: v1
metadata:
  name: tcp-services
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---
kind: ConfigMap
apiVersion: v1
metadata:
  name: udp-services
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nginx-ingress-serviceaccount
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: nginx-ingress-clusterrole
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
rules:
# 添加如下配置
  - apiGroups:
      - "extensions"
      - "networking.k8s.io"
    resources:
      - ingresses
    verbs:
      - list
      - watch
###########end################
  - apiGroups:
      - ""
    resources:
      - configmaps
      - endpoints
      - nodes
      - pods
      - secrets
    verbs:
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - services
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - "extensions"
    resources:
      - ingresses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
  - apiGroups:
      - "extensions"
    resources:
      - ingresses/status
    verbs:
      - update

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: Role
metadata:
  name: nginx-ingress-role
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
      - pods
      - secrets
      - namespaces
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - configmaps
    resourceNames:
      # Defaults to "<election-id>-<ingress-class>"
      # Here: "<ingress-controller-leader>-<nginx>"
      # This has to be adapted if you change either parameter
      # when launching the nginx-ingress-controller.
      - "ingress-controller-leader-nginx"
    verbs:
      - get
      - update
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - create
  - apiGroups:
      - ""
    resources:
      - endpoints
    verbs:
      - get

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: RoleBinding
metadata:
  name: nginx-ingress-role-nisa-binding
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: nginx-ingress-role
subjects:
  - kind: ServiceAccount
    name: nginx-ingress-serviceaccount
    namespace: ingress-nginx

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: nginx-ingress-clusterrole-nisa-binding
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: nginx-ingress-clusterrole
subjects:
  - kind: ServiceAccount
    name: nginx-ingress-serviceaccount
    namespace: ingress-nginx

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-ingress-controller
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
  template:
    metadata:
      labels:
        app.kubernetes.io/name: ingress-nginx
        app.kubernetes.io/part-of: ingress-nginx
      annotations:
        prometheus.io/port: "10254"
        prometheus.io/scrape: "true"
    spec:
      serviceAccountName: nginx-ingress-serviceaccount
      containers:
        - name: nginx-ingress-controller
        # 修改镜像地址
          image: registry.cn-hangzhou.aliyuncs.com/wuji_cyb/ingress-controller:0.25.0
          args:
            - /nginx-ingress-controller
            - --configmap=$(POD_NAMESPACE)/nginx-configuration
            - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services
            - --udp-services-configmap=$(POD_NAMESPACE)/udp-services
            - --publish-service=$(POD_NAMESPACE)/ingress-nginx
            - --annotations-prefix=nginx.ingress.kubernetes.io
          securityContext:
            allowPrivilegeEscalation: true
            capabilities:
              drop:
                - ALL
              add:
                - NET_BIND_SERVICE
            # www-data -> 33
            runAsUser: 33
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          ports:
            - name: http
              containerPort: 80
            - name: https
              containerPort: 443
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10

---

```

#### 下载ingress-nginx 暴露端口配置文件

```shell
wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.25.0/deploy/static/provider/baremetal/service-nodeport.yaml
# 启动
 kubectl apply -f service-nodeport.yaml 
```

```
apiVersion: v2
kind: Service
metadata:
  name: ingress-nginx
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
spec:
  type: NodePort
  ports:
    - name: http
      port: 80
      targetPort: 80
      protocol: TCP
    - name: https
      port: 443
      targetPort: 443
      protocol: TCP
  selector:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
```

```shell
[root@master01 ingress-vh]# kubectl get svc -n ingress-nginx
NAME            TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE
ingress-nginx   NodePort   10.96.109.162   <none>        80:30480/TCP,443:31002/TCP   25m

```

#### 启动deployment和service 通过标签对应到ingress上

```yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: nginx-dm
spec:
  replicas: 2
  template:
    metadata:
      labels:
        name: nginx # 和service 对应
    spec:
      containers:
        - name: nginx
          image: wangyanglinux/myapp:v1
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: nginx-svc
spec:
  ports:
    - port: 80
      targetPort: 80
      protocol: TCP
  selector:
    name: nginx # 和deployment对应
```

ingress配置

```yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: nginx-test
spec:
  rules:
    - host: www1.st.com
      http:
        paths:
        - path: /
          backend:
            serviceName: nginx-svc # 和service对应
            servicePort: 80 
```

#### 多个暴露实例

deployment.yaml

```yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: deployment1
spec:
  replicas: 2
  template:
    metadata:
      labels:
        name: nginx
    spec:
      containers:
        - name: nginx
          image: wangyanglinux/myapp:v1
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: svc-1
spec:
  ports:
    - port: 80
      targetPort: 80
      protocol: TCP
  selector:
    name: nginx

```

deployment2.yaml

```yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: deployment2
spec:
  replicas: 2
  template:
    metadata:
      labels:
        name: nginx2
    spec:
      containers:
        - name: nginx2
          image: wangyanglinux/myapp:v2
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: svc-2
spec:
  ports:
    - port: 80
      targetPort: 80
      protocol: TCP
  selector:
    name: nginx2

```

ingress.yaml

```yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress1
spec:
  rules:
    - host: www1.st.com
      http:
        paths:
        - path: /
          backend:
            serviceName: svc-1
            servicePort: 80
---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress2
spec:
  rules:
    - host: www2.st.com
      http:
        paths:
        - path: /
          backend:
            serviceName: svc-2
            servicePort: 80

```

修改hosts文件

```xml
192.168.1.102 www1.st.com
192.168.1.102 www2.st.com
```

访问域名和ingress-nginx暴露的端口就可以访问到服务

```shell
http://www1.st.com:30480/
```

```
http://www2.st.com:30480/
```

进入容器看到 nginx.conf

```shell
[root@master01 myapp]# kubectl get pod -n ingress-nginx
NAME                                        READY   STATUS    RESTARTS   AGE
nginx-ingress-controller-5b56b5b777-9rp7n   1/1     Running   8          37m
# 进入pod
[root@master01 myapp]# kubectl exec nginx-ingress-controller-5b56b5b777-9rp7n -n ingress-nginx  -it -- /bin/bash
www-data@nginx-ingress-controller-5b56b5b777-9rp7n:/etc/nginx$ cat nginx.conf 
## ....
## start server www1.st.com
	server {
		server_name www1.st.com ;
		
		listen 80;
		
		set $proxy_upstream_name "-";
		set $pass_access_scheme $scheme;
		set $pass_server_port $server_port;
		set $best_http_host $http_host;
		set $pass_port $pass_server_port;
		
		location / {
			
			set $namespace      "default";
			set $ingress_name   "nginx-test";
			set $service_name   "nginx-svc";
			set $service_port   "80";
			set $location_path  "/";
			
			rewrite_by_lua_block {
				lua_ingress.rewrite({
					force_ssl_redirect = false,
					use_port_in_redirects = false,
				})
				balancer.rewrite()
				plugins.run()
			}
			
			header_filter_by_lua_block {
				
				plugins.run()
			}
			body_filter_by_lua_block {
				
			}
			
			log_by_lua_block {
				
				balancer.log()
				
				monitor.call()
				
				plugins.run()
			}
			
			port_in_redirect off;
			
			set $balancer_ewma_score -1;
			set $proxy_upstream_name    "default-nginx-svc-80";
			set $proxy_host             $proxy_upstream_name;
			
			set $proxy_alternative_upstream_name "";
			
			client_max_body_size                    1m;
			
			proxy_set_header Host                   $best_http_host;
			
			# Pass the extracted client certificate to the backend
			
			# Allow websocket connections
			proxy_set_header                        Upgrade           $http_upgrade;
			
			proxy_set_header                        Connection        $connection_upgrade;
			
			proxy_set_header X-Request-ID           $req_id;
			proxy_set_header X-Real-IP              $the_real_ip;
			
			proxy_set_header X-Forwarded-For        $the_real_ip;
			
			proxy_set_header X-Forwarded-Host       $best_http_host;
			proxy_set_header X-Forwarded-Port       $pass_port;
			proxy_set_header X-Forwarded-Proto      $pass_access_scheme;
			
			proxy_set_header X-Original-URI         $request_uri;
			
			proxy_set_header X-Scheme               $pass_access_scheme;
			
			# Pass the original X-Forwarded-For
			proxy_set_header X-Original-Forwarded-For $http_x_forwarded_for;
			
			# mitigate HTTPoxy Vulnerability
			# https://www.nginx.com/blog/mitigating-the-httpoxy-vulnerability-with-nginx/
			proxy_set_header Proxy                  "";
			
			# Custom headers to proxied server
			
			proxy_connect_timeout                   5s;
			proxy_send_timeout                      60s;
			proxy_read_timeout                      60s;
			
			proxy_buffering                         off;
			proxy_buffer_size                       4k;
			proxy_buffers                           4 4k;
			proxy_request_buffering                 on;
			
			proxy_http_version                      1.1;
			
			proxy_cookie_domain                     off;
			proxy_cookie_path                       off;
			
			# In case of errors try the next upstream server before returning an error
			proxy_next_upstream                     error timeout;
			proxy_next_upstream_timeout             0;
			proxy_next_upstream_tries               3;
			
			proxy_pass http://upstream_balancer;
			
			proxy_redirect                          off;
			
		}
		
	}
	## end server www1.st.com
	
	## start server www2.st.com
	server {
		server_name www2.st.com ;
		
		listen 80;
		
		set $proxy_upstream_name "-";
		set $pass_access_scheme $scheme;
		set $pass_server_port $server_port;
		set $best_http_host $http_host;
		set $pass_port $pass_server_port;
		
		location / {
			
			set $namespace      "default";
			set $ingress_name   "ingress2";
			set $service_name   "svc-2";
			set $service_port   "80";
			set $location_path  "/";
			
			rewrite_by_lua_block {
				lua_ingress.rewrite({
					force_ssl_redirect = false,
					use_port_in_redirects = false,
				})
				balancer.rewrite()
				plugins.run()
			}
			
			header_filter_by_lua_block {
				
				plugins.run()
			}
			body_filter_by_lua_block {
				
			}
			
			log_by_lua_block {
				
				balancer.log()
				
				monitor.call()
				
				plugins.run()
			}
			
			port_in_redirect off;
			
			set $balancer_ewma_score -1;
			set $proxy_upstream_name    "default-svc-2-80";
			set $proxy_host             $proxy_upstream_name;
			
			set $proxy_alternative_upstream_name "";
			
			client_max_body_size                    1m;
			
			proxy_set_header Host                   $best_http_host;
			
			# Pass the extracted client certificate to the backend
			
			# Allow websocket connections
			proxy_set_header                        Upgrade           $http_upgrade;
			
			proxy_set_header                        Connection        $connection_upgrade;
			
			proxy_set_header X-Request-ID           $req_id;
			proxy_set_header X-Real-IP              $the_real_ip;
			
			proxy_set_header X-Forwarded-For        $the_real_ip;
			
			proxy_set_header X-Forwarded-Host       $best_http_host;
			proxy_set_header X-Forwarded-Port       $pass_port;
			proxy_set_header X-Forwarded-Proto      $pass_access_scheme;
			
			proxy_set_header X-Original-URI         $request_uri;
			
			proxy_set_header X-Scheme               $pass_access_scheme;
			
			# Pass the original X-Forwarded-For
			proxy_set_header X-Original-Forwarded-For $http_x_forwarded_for;
			
			# mitigate HTTPoxy Vulnerability
			# https://www.nginx.com/blog/mitigating-the-httpoxy-vulnerability-with-nginx/
			proxy_set_header Proxy                  "";
			
			# Custom headers to proxied server
			
			proxy_connect_timeout                   5s;
			proxy_send_timeout                      60s;
			proxy_read_timeout                      60s;
			
			proxy_buffering                         off;
			proxy_buffer_size                       4k;
			proxy_buffers                           4 4k;
			proxy_request_buffering                 on;
			
			proxy_http_version                      1.1;
			
			proxy_cookie_domain                     off;
			proxy_cookie_path                       off;
			
			# In case of errors try the next upstream server before returning an error
			proxy_next_upstream                     error timeout;
			proxy_next_upstream_timeout             0;
			proxy_next_upstream_tries               3;
			
			proxy_pass http://upstream_balancer;
			
			proxy_redirect                          off;
			
		}
		
	}
	## end server www2.st.com
```

Ingress HTTPS 代理访问

```shell
# 创建证书
req -x509 -sha256 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj "/CN=nginxsvc/O=nginxsvc"
# 加载证书
kubectl create secret tls tls-secret --key tls.key --cert tls.crt
```

deployment3.yaml

```yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: deployment3
spec:
  replicas: 2
  template:
    metadata:
      labels:
        name: nginx3
    spec:
      containers:
        - name: nginx3
          image: wangyanglinux/myapp:v1
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: svc-3
spec:
  ports:
    - port: 80
      targetPort: 80
      protocol: TCP
  selector:
    name: nginx3
```

ingress-https.yaml

```yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: https
spec:
  tls:
    - hosts:
      - www3.st.com
      secretName: tls-secret
  rules:
    - host: www3.st.com
      http:
        paths:
        - path: /
          backend:
            serviceName: svc-3
            servicePort: 80

```

查看端口

```shell
[root@master01 ingress-https]# kubectl get svc -n ingress-nginx
NAME            TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE
ingress-nginx   NodePort   10.96.109.162   <none>        80:30480/TCP,443:31002/TCP   42m

```

修改host文件

```xml
192.168.1.102 www3.st.com
```

访问地址

```shell
https://www3.st.com:31002/
```

#### Nginx 惊醒 BasicAuth

```shell
# 安装httppd
yum -y install httpd
# 创建用户密码 -c create auth：文件名 foo:用户 然后输入密码
htpasswd -c auth foo
kubectl create secret generic basic-auth --from-file=auth
```



```yaml

apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-with-auth
  annotations:
    nginx.ingress.kubernetes.io/auth-type: basic
    nginx.ingress.kubernetes.io/auth-secret: basic-auth
    nginx.ingress.kubernetes.io/auth-realm: 'Authentication Required - foo'
spec:
  rules:
  - host: www4.st.com
    http:
      paths:
      - path: /
        backend:
          serviceName: nginx-svc
          servicePort: 80
```

|                     名称                      | 描述                                                         | 值   |
| :-------------------------------------------: | ------------------------------------------------------------ | ---- |
|   nginx.ingress.kubernetes.io/rewritetarget   | 必须重定向流量的目标URI                                      | 串   |
|    nginx.ingress.kubernetes.io/sslredirect    | 指示位置部分是否仅可访问SSL（当Ingress包含证书时 默认为True） | 布尔 |
| nginx.ingress.kubernetes.io/forcessl-redirect | 即使Ingress未启用TLS，也强制重定向到HTTPS                    | 布尔 |
|      nginx.ingress.kubernetes.io/approot      | 定义Controller必须重定向的应用程序根，如果它在'/'上 下文中   | 串   |
|     nginx.ingress.kubernetes.io/useregex      | 指示Ingress上定义的路径是否使用正则表达式                    | 布尔 |

#### 地址跳转

```yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: nginx-test
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: https://www3.st.com:31002/
spec:
  rules:
  - host: www5.st.com
    http:
      paths:
      - path: /
        backend:
          serviceName: nginx-svc
          servicePort: 80
```

访问 www5.st.com 能够跳转到https://www3.st.com:31002/ 上

